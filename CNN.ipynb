{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "GeneCNN, a convolutional neural network-based gene predictor\n",
    "Developed by Michael Morikone\n",
    "Requires a genome file and transcriptomic data\n",
    "Tested with the masked flattened buffalograss genome and 6 transcriptomic datasets from 4 different BioProjects\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from Bio import SeqIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "genome = SeqIO.parse(\"flattened_genome_model_mask.fasta\", \"fasta\") #single sequence masked genome used for training dataset\n",
    "\n",
    "seqs = []\n",
    "seqs_ids = []\n",
    "\n",
    "#converts SeqIO object to list\n",
    "for sequences in genome:\n",
    "    seqs.append(str(sequences.seq))\n",
    "    seqs_ids.append(sequences.id)\n",
    "\n",
    "#flatten genome and create associated vector with scaffold assignment per base\n",
    "flattened_genome = []\n",
    "genome_id = [] \n",
    "\n",
    "for i in range(len(seqs_ids)):\n",
    "    for j in range(len(seqs[i])):\n",
    "        flattened_genome.append(seqs[i][j])\n",
    "        genome_id.append(seqs_ids[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Mapped all transcriptomic data to flattened cleaned genome.\n",
    "#Created sorted bam files and used samtools depth -aa to generate mapping depth per base and put into a file with depth per mapping file as columns.\n",
    "#Iterate through lines and iterate through columns. Add all depths to a list, if sum of list is 0, add 0 to true value list. If sum is >0, add 1 to true value list.\n",
    "\n",
    "true_value = []\n",
    "with open(\"flattened_genome_model_mask_samtools_depth.txt\") as f:\n",
    "    for line in f:\n",
    "        result = []\n",
    "        result.append(line.split('\\t')[2:]) #take only depth\n",
    "        result[0][-1] = result[0][-1].strip() #remove new line\n",
    "        result[0] = [int(i) for i in result[0]] #convert list to int\n",
    "        if sum(result[0])>0:\n",
    "            true_value.append(1)\n",
    "        else:\n",
    "            true_value.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Creation of label lists for dataset\n",
    "\n",
    "#single sample length\n",
    "no_length = 0 \n",
    "yes_length = 0\n",
    "\n",
    "#single sample sequence\n",
    "no_current = []\n",
    "yes_current = []\n",
    "\n",
    "#all samples\n",
    "no_list = []\n",
    "yes_list = []\n",
    "sequence_length = 500 #highest performance tested from 200-800 bp\n",
    "\n",
    "for x in range(len(true_value)):\n",
    "    if no_length < sequence_length and yes_length < sequence_length:\n",
    "        if true_value[x] == 0:\n",
    "            no_length += 1\n",
    "            no_current.append(flattened_genome[x])\n",
    "            yes_length = 0\n",
    "            yes_current = []\n",
    "        elif true_value[x] == 1:\n",
    "            yes_length += 1\n",
    "            yes_current.append(flattened_genome[x])\n",
    "            no_length = 0\n",
    "            no_current = []\n",
    "    else:\n",
    "        if no_length == sequence_length:\n",
    "            no_list.append(no_current)\n",
    "            no_current = []\n",
    "            no_length = 0\n",
    "        elif yes_length == sequence_length:\n",
    "            yes_list.append(yes_current)\n",
    "            yes_current = []\n",
    "            yes_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampling, better performance than undersampling\n",
    "\n",
    "complement = len(no_list) - len(yes_list)\n",
    "yes_sample = []\n",
    "yes_sample.extend(yes_list)\n",
    "for i in range(complement):\n",
    "    yes_sample.extend(random.choices(yes_list, k=1)) #keeps original dataset then adds nearly 4x randomly selected data of yes_list to create positive labeled dataset of same size as negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#creation of flattened labels to match flatened dataset, in order of all positive samples then all negative samples in one list\n",
    "\n",
    "#samples\n",
    "merged_list = []\n",
    "merged_list.extend(yes_sample) \n",
    "merged_list.extend(no_list) \n",
    "\n",
    "#create labels\n",
    "yes_labels = [1] * len(yes_sample)\n",
    "no_labels = [0] * len(no_list) \n",
    "\n",
    "#combine and keep same order as samples\n",
    "labels = []\n",
    "labels.extend(yes_labels)\n",
    "labels.extend(no_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# data preparation for one hot encoding, conversion of alphabetic representation of nucleotides to numeric representation\n",
    "\n",
    "ordinal_genome = [] \n",
    "\n",
    "for seq in range(len(merged_list)):\n",
    "    working_seq = []\n",
    "    for nucleotide in range(len(merged_list[seq])):\n",
    "        if merged_list[seq][nucleotide] == 'A':\n",
    "            working_seq.append(0)\n",
    "        elif merged_list[seq][nucleotide] == 'C':\n",
    "            working_seq.append(1)\n",
    "        elif merged_list[seq][nucleotide] == 'G':\n",
    "            working_seq.append(2)\n",
    "        else:\n",
    "            working_seq.append(3)\n",
    "    ordinal_genome.append(working_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#data splitting\n",
    "\n",
    "train_ratio = 0.75\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.10\n",
    "\n",
    "# train is now 75% of the entire data set\n",
    "x_train, x_test, y_train, y_test = train_test_split(ordinal_genome, labels, test_size=1 - train_ratio)\n",
    "\n",
    "# test is now 10% of the initial data set\n",
    "# validation is now 15% of the initial data set\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# one hot encoding for data and labels\n",
    "genome_category_count = 4\n",
    "binary_train= tf.one_hot(x_train, genome_category_count)\n",
    "binary_val= tf.one_hot(x_val, genome_category_count)\n",
    "binary_test= tf.one_hot(x_test, genome_category_count)\n",
    "\n",
    "# Convert both true values to Tensors\n",
    "label_category_count = 2\n",
    "\n",
    "y_train = tf.one_hot(y_train, label_category_count)\n",
    "y_val = tf.one_hot(y_val, label_category_count)\n",
    "y_test = tf.one_hot(y_test, label_category_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirm shaping is correct\n",
    "\n",
    "print(binary_train.shape)\n",
    "print(tf.convert_to_tensor(y_train).shape)\n",
    "print(binary_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#model definition for manual tweaking, uncomment if needed and comment out next three code blocks instead\n",
    "\n",
    "'''#Modeling\n",
    "from tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, Flatten, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=10, kernel_size=3, input_shape=(binary_train.shape[1], 4), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(Conv1D(filters=20, kernel_size=3, activation='relu')) \n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(Conv1D(filters=40, kernel_size=5, activation='relu')) \n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=['binary_accuracy'])\n",
    "model.summary()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model definition for tuning\n",
    "\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential([\n",
    "    keras.layers.Conv1D(filters=hp.Int('conv1_filter', min_value=4, max_value=128, step=4), kernel_size=hp.Choice('conv1_kernel', values=[3,5]), activation='relu', input_shape=(binary_train.shape[1], 4)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling1D(pool_size=4),\n",
    "    keras.layers.Conv1D(filters=hp.Int('conv2_filter', min_value=4, max_value=128, step=4), kernel_size=hp.Choice('conv2_kernel', values=[3,5]), activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling1D(pool_size=4),\n",
    "    keras.layers.Conv1D(filters=hp.Int('conv3_filter', min_value=4, max_value=128, step=4), kernel_size=hp.Choice('conv3_kernel', values=[3,5]), activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling1D(pool_size=4),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(2, activation='softmax')])\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-1, 1e-2, 1e-3])), loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BayesianOptimization tuning\n",
    "\n",
    "from keras_tuner import BayesianOptimization\n",
    "\n",
    "tuner = BayesianOptimization(build_model, objective='val_binary_accuracy', max_trials=15, project_name='Bayesian2')\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "tuner.search(binary_train, y_train, epochs=20, validation_data=(binary_val,y_val), callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model layout\n",
    "\n",
    "model=tuner.get_best_models(num_models=1)[0]\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model training and loss plot\n",
    "\n",
    "history = model.fit(binary_train, y_train, epochs=20, validation_data=(binary_val,y_val))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Accuracy plot\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['binary_accuracy'])\n",
    "plt.plot(history.history['val_binary_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test loss and accuracy\n",
    "results = model.evaluate(binary_test, y_test)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "predicted_labels = model.predict(np.stack(binary_test))\n",
    "cm = confusion_matrix(np.argmax(y_test, axis=1),\n",
    "                      np.argmax(predicted_labels, axis=1))\n",
    "print('Confusion matrix:\\n',cm)\n",
    "\n",
    "cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n",
    "\n",
    "plt.imshow(cm, cmap=plt.cm.Blues)\n",
    "plt.title('Normalized confusion matrix')\n",
    "plt.colorbar()\n",
    "plt.xlabel('True label')\n",
    "plt.ylabel('Predicted label')\n",
    "plt.xticks([0, 1]); plt.yticks([0, 1])\n",
    "plt.grid('off')\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], '.2f'),\n",
    "             horizontalalignment='center',\n",
    "             color='white' if cm[i, j] > 0.5 else 'black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#F1 score\n",
    "loss, f1 = model.evaluate(binary_test, y_test, verbose=0)\n",
    "print('F1 Score: %.3f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "model.save(\"Bayesian2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use to visualize model layout as a figure\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import pydot, graphviz\n",
    "plot_model(model, to_file='model_plot_noNone.png', show_shapes=True, show_layer_names=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow-gpu-2.9-custom)",
   "language": "python",
   "name": "conda_default_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
